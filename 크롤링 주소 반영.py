import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import copy
import traceback

# ğŸ”¹ ë°œí–‰êµ­ ë¶€í˜¸ êµ¬í•˜ê¸° (êµ¬ê¸€ ì‹œíŠ¸ Sheet2 í™œìš©)
def get_country_code_by_region(region_name):
    try:
        st.write(f"ğŸŒ ë°œí–‰êµ­ ë¶€í˜¸ ì°¾ëŠ” ì¤‘... ì°¸ì¡° ì§€ì—­: `{region_name}`")

        json_key = dict(st.secrets["gspread"])
        json_key["private_key"] = json_key["private_key"].replace('\\n', '\n')

        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive"
        ]
        creds = ServiceAccountCredentials.from_json_keyfile_dict(json_key, scope)
        client = gspread.authorize(creds)
        sheet = client.open("ì¶œíŒì‚¬ DB").worksheet("Sheet2")

        region_col = sheet.col_values(1)[1:]  # Aì—´: ì§€ì—­ëª… (ê¸°ì¤€)
        code_col = sheet.col_values(2)[1:]    # Bì—´: ë°œí–‰êµ­ ë¶€í˜¸

        # âœ… ì •ê·œí™” í•¨ìˆ˜
        def normalize_region(region):
            region = region.strip()
            original = region

            # 1. íŠ¹ë³„ìì¹˜ë„ ì œê±° (ë‹¨, ë”°ë¡œ í‘œì‹œí•´ ê¸°ì–µ)
            was_teukbyeol = "íŠ¹ë³„ìì¹˜ë„" in region
            region = re.sub(r"(ê´‘ì—­ì‹œ|íŠ¹ë³„ì‹œ|íŠ¹ë³„ìì¹˜ë„)", "", region)

            # 2. ì˜ˆì™¸ ì²˜ë¦¬
            if region in ["ê°•ì›ë„", "ì œì£¼ë„", "ê²½ê¸°ë„"]:
                return region.replace("ë„", "")

            # 3. ~ë„ ì²˜ë¦¬ (íŠ¹ë³„ìì¹˜ë„ì˜€ë˜ í•­ëª©ì€ ì—¬ê¸°ì„œ ì œì™¸)
            if region.endswith("ë„") and len(region) >= 4 and not was_teukbyeol:
                return region[0] + region[2]

            # 4. ~ì‹œ ì²˜ë¦¬
            if region.endswith("ì‹œ"):
                return region[:-1]

            return region

            

        normalized_input = normalize_region(region_name)
        st.write(f"ğŸ§ª ì •ê·œí™”ëœ ì°¸ì¡°ì§€ì—­: `{normalized_input}`")

        for sheet_region, country_code in zip(region_col, code_col):
            if normalize_region(sheet_region) == normalized_input:
                return country_code.strip() or "xxu"

        return "xxu"  # ë¯¸ìƒ

    except Exception as e:
        return "xxu"


# ğŸ”¹ Google Sheetsì—ì„œ ì§€ì—­ëª… ì¶”ì¶œ (ë””ë²„ê¹… í¬í•¨)
def get_publisher_location(publisher_name):
    try:
        st.write(f"ğŸ“¥ ì¶œíŒì‚¬ ì§€ì—­ì„ êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
        st.write(f"ğŸ” ì…ë ¥ëœ ì¶œíŒì‚¬ëª…: `{publisher_name}`")

        # âœ… st.secretsëŠ” dictë¡œ ë³€í™˜ (deepcopy ê¸ˆì§€)
        json_key = dict(st.secrets["gspread"])
        json_key["private_key"] = json_key["private_key"].replace('\\n', '\n')

        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive"
        ]
        creds = ServiceAccountCredentials.from_json_keyfile_dict(json_key, scope)
        client = gspread.authorize(creds)
        sheet = client.open("ì¶œíŒì‚¬ DB").worksheet("Sheet1")

        publisher_names = sheet.col_values(2)[1:]  # Bì—´
        regions = sheet.col_values(3)[1:]          # Cì—´

        def normalize(name):
            return re.sub(r"\s|\(.*?\)|ì£¼ì‹íšŒì‚¬|ãˆœ|ë„ì„œì¶œíŒ|ì¶œíŒì‚¬", "", name).lower()

        target = normalize(publisher_name)
        st.write(f"ğŸ§ª ì •ê·œí™”ëœ ì…ë ¥ê°’: `{target}`")

        preview_names = [normalize(name) for name in publisher_names[:10]]
        st.write(f"ğŸ“‹ êµ¬ê¸€ ì‹œíŠ¸ ë‚´ ì¶œíŒì‚¬ ì •ê·œí™” ë¦¬ìŠ¤íŠ¸ (ìƒìœ„ 10ê°œ): `{preview_names}`")

        for sheet_name, region in zip(publisher_names, regions):
            if normalize(sheet_name) == target:
                return region.strip() or "ì¶œíŒì§€ ë¯¸ìƒ"

        for sheet_name, region in zip(publisher_names, regions):
            if sheet_name.strip() == publisher_name.strip():
                return region.strip() or "ì¶œíŒì§€ ë¯¸ìƒ"

        return "ì¶œíŒì§€ ë¯¸ìƒ"

    except Exception as e:
        return f"ì˜ˆì™¸ ë°œìƒ: {str(e)}"


# ğŸ”¹ ì•Œë¼ë”˜ ìƒì„¸ í˜ì´ì§€ íŒŒì‹± (í˜•íƒœì‚¬í•­ í¬í•¨)
def parse_aladin_detail_page(html):
    soup = BeautifulSoup(html, "html.parser")
    title_tag = soup.select_one("span.Ere_bo_title")
    title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"

    li_tag = soup.select_one("li.Ere_sub2_title")
    author_list = []
    publisher = ""
    pubyear = ""

    if li_tag:
        children = li_tag.contents
        last_a_before_date = None

        for i, node in enumerate(children):
            if getattr(node, "name", None) == "a":
                name = node.text.strip()
                next_text = children[i+1].strip() if i+1 < len(children) and isinstance(children[i+1], str) else ""
                if "ì§€ì€ì´" in next_text:
                    author_list.append(f"{name} ì§€ìŒ")
                elif "ì˜®ê¸´ì´" in next_text:
                    author_list.append(f"{name} ì˜®ê¹€")
                else:
                    last_a_before_date = name
            elif isinstance(node, str):
                date_match = re.search(r"\d{4}-\d{2}-\d{2}", node)
                if date_match:
                    pubyear = date_match.group().split("-")[0]
                    if last_a_before_date:
                        publisher = last_a_before_date

    # âœ… í˜•íƒœì‚¬í•­ ì •ë³´ ì¶”ì¶œ
    form_wrap = soup.select_one("div.conts_info_list1")
    a_part = ""
    c_part = ""

    if form_wrap:
        form_items = [item.strip() for item in form_wrap.stripped_strings]
        
        for item in form_items:
            if re.search(r"(ìª½|p)\s*$", item):
                page_match = re.search(r"\d+", item)
                if page_match:
                    a_part = f"{page_match.group()} p."
            elif "mm" in item:
                size_match = re.search(r"(\d+)\s*[\*xÃ—X]\s*(\d+)", item)
                if size_match:
                    width = int(size_match.group(1))
                    height = int(size_match.group(2))
                    if width == height or width > height or width < height / 2:
                        w_cm = round(width / 10)
                        h_cm = round(height / 10)
                        c_part = f"{w_cm}x{h_cm} cm"
                    else:
                        h_cm = round(height / 10)
                        c_part = f"{h_cm} cm"

    if a_part or c_part:
        field_300 = "=300  \\\\$a"
        if a_part:
            field_300 += a_part
        if c_part:
            field_300 += f" ;$c{c_part}."
    else:
        field_300 = "=300  \\$a1ì±…."

    creator_str = " ; ".join(author_list) if author_list else "ì €ì ì •ë³´ ì—†ìŒ"
    publisher = publisher if publisher else "ì¶œíŒì‚¬ ì •ë³´ ì—†ìŒ"
    pubyear = pubyear if pubyear else "ë°œí–‰ì—°ë„ ì—†ìŒ"

    return {
        "title": title,
        "creator": creator_str,
        "publisher": publisher,
        "pubyear": pubyear,
        "245": f"=245  10$a{title} /$c{creator_str}",
        "300": field_300
    }

# ğŸ”¹ ì•Œë¼ë”˜ ISBN ê²€ìƒ‰
def search_aladin_by_isbn(isbn):
    search_url = f"https://www.aladin.co.kr/search/wsearchresult.aspx?SearchWord={isbn}"
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        res = requests.get(search_url, headers=headers)
        if res.status_code != 200:
            return None, f"ê²€ìƒ‰ ì‹¤íŒ¨ (status {res.status_code})"

        soup = BeautifulSoup(res.text, "html.parser")
        link_tag = soup.select_one("div.ss_book_box a.bo3")
        if not link_tag or not link_tag.get("href"):
            return None, "ë„ì„œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        detail_url = link_tag["href"]
        detail_res = requests.get(detail_url, headers=headers)
        if detail_res.status_code != 200:
            return None, f"ìƒì„¸í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ (status {detail_res.status_code})"

        result = parse_aladin_detail_page(detail_res.text)
        return result, None

    except Exception as e:
        return None, f"ì˜ˆì™¸ ë°œìƒ: {str(e)}"

# ğŸ”¹ Streamlit UI
st.title("ğŸ“š ISBN â†’ í¬ë¡¤ë§ â†’ KORMARC ë³€í™˜ê¸° ğŸ˜‚")

isbn_input = st.text_area("ISBNì„ '/'ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•˜ì„¸ìš”:")

if isbn_input:
    isbn_list = [
        re.sub(r"[^\d]", "", isbn)  # âœ… ìˆ«ìë§Œ ë‚¨ê¹€: 979-11-94244-18-9 â†’ 9791194244189
        for isbn in isbn_input.split("/")
        if isbn.strip()
    ]

    for idx, isbn in enumerate(isbn_list, 1):
        st.markdown(f"---\n### ğŸ“˜ {idx}. ISBN: `{isbn}`")
        with st.spinner("ğŸ” ë„ì„œ ì •ë³´ ê²€ìƒ‰ ì¤‘..."):
            result, error = search_aladin_by_isbn(isbn)

        if error:
            st.error(f"âŒ ì˜¤ë¥˜: {error}")
            continue

        if result:
            publisher = result["publisher"]
            pubyear = result["pubyear"]

            # 245 í•„ë“œ ë¨¼ì € ì¶œë ¥
            st.code(result["245"], language="text")

            # 260 í•„ë“œ êµ¬ì„±
            if publisher == "ì¶œíŒì‚¬ ì •ë³´ ì—†ìŒ":
                location = "[ì¶œíŒì§€ ë¯¸ìƒ]"
            else:
                with st.spinner(f"ğŸ“ '{publisher}'ì˜ ì§€ì—­ì •ë³´ ê²€ìƒ‰ ì¤‘..."):
                    location = get_publisher_location(publisher)

            # ë””ë²„ê¹… or ì§€ì—­ì •ë³´ ë©”ì‹œì§€ (ê°€ì¥ ë§ˆì§€ë§‰)
            if publisher != "ì¶œíŒì‚¬ ì •ë³´ ì—†ìŒ":
                st.info(f"ğŸ™ï¸ ì§€ì—­ì •ë³´ ê²°ê³¼: **{location}**")

            # 260 í•„ë“œ ì¶œë ¥
            updated_260 = f"=260  \\$a{location} :$b{publisher},$c{pubyear}."
            st.code(updated_260, language="text")  

            # 300 í•„ë“œ ì¶œë ¥
            st.code(result["300"], language="text")
            
            # 008 í•„ë“œ ì¶œë ¥ (ë°œí–‰êµ­ ë¶€í˜¸)
            country_code = get_country_code_by_region(location)
            field_008 = f"=008  \\\\$a{country_code}"
            st.code(field_008, language="text")


        else:
            st.warning("ê²°ê³¼ ì—†ìŒ")
