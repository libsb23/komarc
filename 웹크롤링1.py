import streamlit as st
import requests
from bs4 import BeautifulSoup
import re

def crawl_aladin_book_info(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers)
        if res.status_code != 200:
            return None, f"ìƒì„¸ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: status code {res.status_code}"

        soup = BeautifulSoup(res.text, "html.parser")

        # ì œëª©
        title_tag = soup.select_one("span.Ere_bo_title")
        title = title_tag.text.strip() if title_tag else None
        if not title:
            return None, "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (span.Ere_bo_title ì„ íƒ ì‹¤íŒ¨)"

        # ì €ì / ì—­ì
        creator_tags = soup.select("li.Ere_sub2_title a")
        creators = [tag.text.strip() for tag in creator_tags]
        creator_str = " ; ".join(creators) if creators else None
        if not creator_str:
            return None, "ì €ì ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (li.Ere_sub2_title > a ì„ íƒ ì‹¤íŒ¨)"

        # ì¶œíŒì‚¬ ë° ë°œí–‰ì—°ë„
        pub_info_tag = soup.select_one("li.Ere_sub_content")
        if not pub_info_tag:
            return None, "ì¶œíŒì‚¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (li.Ere_sub_content ì„ íƒ ì‹¤íŒ¨)"

        pub_text = pub_info_tag.text.strip()
        publisher = ""
        pubyear = ""

        if "/" in pub_text:
            parts = pub_text.split("/")
            publisher = parts[0].strip()
            year_match = re.search(r"\d{4}", parts[1])
            pubyear = year_match.group() if year_match else ""
        else:
            publisher = pub_text.strip()

        result = {
            "245": f"=245  10$a{title} /$c{creator_str}",
            "260": f"=260  \\$a[ì¶œíŒì§€ ë¯¸ìƒ] :$b{publisher},$c{pubyear}.",
            "300": f"=300  \\$a1ì±…."
        }
        return result, None

    except Exception as e:
        return None, f"ì˜ˆì™¸ ë°œìƒ: {str(e)}"

def search_aladin_by_isbn(isbn):
    search_url = f"https://www.aladin.co.kr/search/wsearchresult.aspx?SearchWord={isbn}"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(search_url, headers=headers)
        if res.status_code != 200:
            return None, f"ê²€ìƒ‰ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: status code {res.status_code}"

        soup = BeautifulSoup(res.text, "html.parser")
        link_tag = soup.select_one("div.ss_book_box a.bo3")
        if not link_tag or not link_tag.get("href"):
            return None, "ë„ì„œ ìƒì„¸ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (a.bo3 ì„ íƒ ì‹¤íŒ¨)"

        detail_url = link_tag["href"]
        return crawl_aladin_book_info(detail_url)

    except Exception as e:
        return None, f"ì˜ˆì™¸ ë°œìƒ: {str(e)}"

# Streamlit UI
st.title("ğŸ“š ì•Œë¼ë”˜ KORMARC í•„ë“œ ì¶”ì¶œê¸°")

isbn = st.text_input("ISBNì„ ì…ë ¥í•˜ì„¸ìš”:")

if isbn:
    with st.spinner("ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤..."):
        result, error = search_aladin_by_isbn(isbn)
        if error:
            st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {error}")
        elif result:
            st.subheader("ğŸ“„ KORMARC í•„ë“œ")
            st.text(result["245"])
            st.text(result["260"])
            st.text(result["300"])
        else:
            st.warning("ë„ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
