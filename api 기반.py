import streamlit as st
import requests
import re
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from bs4 import BeautifulSoup

# --- ë°œí–‰êµ­ ë¶€í˜¸ êµ¬í•˜ê¸° (êµ¬ê¸€ ì‹œíŠ¸ Sheet2 í™œìš©) ---
def get_country_code_by_region(region_name):
    try:
        st.write(f"ðŸŒ ë°œí–‰êµ­ ë¶€í˜¸ ì°¾ëŠ” ì¤‘... ì°¸ì¡° ì§€ì—­: `{region_name}`")

        json_key = dict(st.secrets["gspread"])
        json_key["private_key"] = json_key["private_key"].replace('\\n', '\n')

        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive"
        ]
        creds = ServiceAccountCredentials.from_json_keyfile_dict(json_key, scope)
        client = gspread.authorize(creds)
        sheet = client.open("ì¶œíŒì‚¬ DB").worksheet("Sheet2")

        region_col = sheet.col_values(1)[1:]
        code_col = sheet.col_values(2)[1:]

        def normalize_region(region):
            region = region.strip()
            was_teukbyeol = "íŠ¹ë³„ìžì¹˜ë„" in region
            region = re.sub(r"(ê´‘ì—­ì‹œ|íŠ¹ë³„ì‹œ|íŠ¹ë³„ìžì¹˜ë„)", "", region)
            if region in ["ê°•ì›ë„", "ì œì£¼ë„", "ê²½ê¸°ë„"]:
                return region.replace("ë„", "")
            if region.endswith("ë„") and len(region) >= 4 and not was_teukbyeol:
                return region[0] + region[2]
            if region.endswith("ì‹œ"):
                return region[:-1]
            return region

        normalized_input = normalize_region(region_name)
        st.write(f"ðŸ§ª ì •ê·œí™”ëœ ì°¸ì¡°ì§€ì—­: `{normalized_input}`")

        for sheet_region, country_code in zip(region_col, code_col):
            if normalize_region(sheet_region) == normalized_input:
                return country_code.strip() or "xxu"

        return "xxu"

    except Exception:
        return "xxu"

# --- Google Sheetsì—ì„œ ì¶œíŒì‚¬ ì§€ì—­ëª… ì¶”ì¶œ ---
def get_publisher_location(publisher_name):
    try:
        st.write(f"ðŸ“¥ ì¶œíŒì‚¬ ì§€ì—­ì„ êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ì°¾ëŠ” ì¤‘ìž…ë‹ˆë‹¤... `{publisher_name}`")

        json_key = dict(st.secrets["gspread"])
        json_key["private_key"] = json_key["private_key"].replace('\\n', '\n')

        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/spreadsheets",
            "https://www.googleapis.com/auth/drive"
        ]
        creds = ServiceAccountCredentials.from_json_keyfile_dict(json_key, scope)
        client = gspread.authorize(creds)
        sheet = client.open("ì¶œíŒì‚¬ DB").worksheet("Sheet1")

        publisher_names = sheet.col_values(2)[1:]
        regions = sheet.col_values(3)[1:]

        def normalize(name):
            return re.sub(r"\s|\(.*?\)|ì£¼ì‹íšŒì‚¬|ãˆœ|ë„ì„œì¶œíŒ|ì¶œíŒì‚¬", "", name).lower()

        target = normalize(publisher_name)
        st.write(f"ðŸ§ª ì •ê·œí™”ëœ ìž…ë ¥ê°’: `{target}`")

        for sheet_name, region in zip(publisher_names, regions):
            if normalize(sheet_name) == target:
                return region.strip() or "ì¶œíŒì§€ ë¯¸ìƒ"

        for sheet_name, region in zip(publisher_names, regions):
            if sheet_name.strip() == publisher_name.strip():
                return region.strip() or "ì¶œíŒì§€ ë¯¸ìƒ"

        return "ì¶œíŒì§€ ë¯¸ìƒ"

    except Exception:
        return "ì˜ˆì™¸ ë°œìƒ"

# --- API ê¸°ë°˜ ë„ì„œì •ë³´ ê°€ì ¸ì˜¤ê¸° ---
def search_aladin_by_isbn(isbn):
    try:
        ttbkey = st.secrets["aladin"]["ttbkey"]
        url = "https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
        params = {
            "ttbkey": ttbkey,
            "itemIdType": "ISBN",
            "ItemId": isbn,
            "output": "js",
            "Version": "20131101"
        }

        res = requests.get(url, params=params)
        if res.status_code != 200:
            return None, f"API ìš”ì²­ ì‹¤íŒ¨ (status: {res.status_code})"

        data = res.json()
        if "item" not in data or not data["item"]:
            return None, f"ë„ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. [ì‘ë‹µ ë‚´ìš©: {data}]"

        book = data["item"][0]

        title = book.get("title", "ì œëª© ì—†ìŒ")
        author = book.get("author", "")
        publisher = book.get("publisher", "ì¶œíŒì‚¬ ì •ë³´ ì—†ìŒ")
        pubdate = book.get("pubDate", "")
        pubyear = pubdate[:4] if len(pubdate) >= 4 else "ë°œí–‰ë…„ë„ ì—†ìŒ"

        authors = [a.strip() for a in author.split(",")]
        creator_str = " ; ".join(authors) if authors else "ì €ìž ì •ë³´ ì—†ìŒ"

        field_245 = f"=245  10$a{title} /$c{creator_str}"

        return {
            "title": title,
            "creator": creator_str,
            "publisher": publisher,
            "pubyear": pubyear,
            "245": field_245
        }, None

    except Exception as e:
        return None, f"API ì˜ˆì™¸ ë°œìƒ: {str(e)}"

# --- í˜•íƒœì‚¬í•­ í¬ë¡¤ë§ ì¶”ì¶œ ---
def extract_physical_description_by_crawling(isbn):
    try:
        search_url = f"https://www.aladin.co.kr/search/wsearchresult.aspx?SearchWord={isbn}"
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(search_url, headers=headers)
        if res.status_code != 200:
            return "=300  \\$a1ì±….", f"ê²€ìƒ‰ ì‹¤íŒ¨ (status {res.status_code})"

        soup = BeautifulSoup(res.text, "html.parser")
        link_tag = soup.select_one("div.ss_book_box a.bo3")
        if not link_tag or not link_tag.get("href"):
            return "=300  \\$a1ì±….", "ë„ì„œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        detail_url = link_tag["href"]
        detail_res = requests.get(detail_url, headers=headers)
        if detail_res.status_code != 200:
            return "=300  \\$a1ì±….", f"ìƒì„¸íŽ˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ (status {detail_res.status_code})"

        detail_soup = BeautifulSoup(detail_res.text, "html.parser")
        form_wrap = detail_soup.select_one("div.conts_info_list1")
        a_part = ""
        c_part = ""

        if form_wrap:
            form_items = [item.strip() for item in form_wrap.stripped_strings]
            for item in form_items:
                if re.search(r"(ìª½|p)\s*$", item):
                    page_match = re.search(r"\d+", item)
                    if page_match:
                        a_part = f"{page_match.group()} p."
                elif "mm" in item:
                    size_match = re.search(r"(\d+)\s*[\*xÃ—X]\s*(\d+)", item)
                    if size_match:
                        width = int(size_match.group(1))
                        height = int(size_match.group(2))
                        if width == height or width > height or width < height / 2:
                            w_cm = round(width / 10)
                            h_cm = round(height / 10)
                            c_part = f"{w_cm}x{h_cm} cm"
                        else:
                            h_cm = round(height / 10)
                            c_part = f"{h_cm} cm"

        if a_part or c_part:
            field_300 = "=300  \\\\$a"
            if a_part:
                field_300 += a_part
            if c_part:
                field_300 += f" ;$c{c_part}."
        else:
            field_300 = "=300  \\$a1ì±…."

        return field_300, None

    except Exception as e:
        return "=300  \\$a1ì±….", f"ì˜ˆì™¸ ë°œìƒ: {str(e)}"


# --- Streamlit UI ---
st.title("ðŸ“š ISBN â†’ API + í¬ë¡¤ë§ â†’ KORMARC ë³€í™˜ê¸°")

isbn_input = st.text_area("ISBNì„ '/'ë¡œ êµ¬ë¶„í•˜ì—¬ ìž…ë ¥í•˜ì„¸ìš”:")

if isbn_input:
    # í•˜ì´í”ˆ, ê³µë°± ë“± ëª¨ë‘ ì œê±°í•˜ê³  ìˆ«ìžë§Œ ì¶”ì¶œ
    isbn_list = [re.sub(r"[^\d]", "", isbn) for isbn in isbn_input.split("/") if isbn.strip()]

    for idx, isbn in enumerate(isbn_list, 1):
        st.markdown(f"---\n### ðŸ“˜ {idx}. ISBN: `{isbn}`")
        with st.spinner("ðŸ” ë„ì„œ ì •ë³´ ê²€ìƒ‰ ì¤‘..."):
            result, error = search_aladin_by_isbn(isbn)

        if error:
            st.error(f"âŒ ì˜¤ë¥˜: {error}")
            continue

        # í˜•íƒœì‚¬í•­ í¬ë¡¤ë§
        with st.spinner("ðŸ“ í˜•íƒœì‚¬í•­ í¬ë¡¤ë§ ì¤‘..."):
            field_300, err_300 = extract_physical_description_by_crawling(isbn)
        if err_300:
            st.warning(f"âš ï¸ í˜•íƒœì‚¬í•­ í¬ë¡¤ë§ ê²½ê³ : {err_300}")

        publisher = result["publisher"]
        pubyear = result["pubyear"]

        # 245 í•„ë“œ ì¶œë ¥
        st.code(result["245"], language="text")

        # 260 í•„ë“œìš© ì¶œíŒì§€ì—­ ì¶”ì¶œ
        if publisher == "ì¶œíŒì‚¬ ì •ë³´ ì—†ìŒ":
            location = "[ì¶œíŒì§€ ë¯¸ìƒ]"
        else:
            with st.spinner(f"ðŸ“ '{publisher}'ì˜ ì§€ì—­ì •ë³´ ê²€ìƒ‰ ì¤‘..."):
                location = get_publisher_location(publisher)

        if publisher != "ì¶œíŒì‚¬ ì •ë³´ ì—†ìŒ":
            st.info(f"ðŸ™ï¸ ì§€ì—­ì •ë³´ ê²°ê³¼: **{location}**")

        # 260 í•„ë“œ ì¶œë ¥
        field_260 = f"=260  \\$a{location} :$b{publisher},$c{pubyear}."
        st.code(field_260, language="text")

        # 300 í•„ë“œ ì¶œë ¥ (í¬ë¡¤ë§ ê²°ê³¼)
        st.code(field_300, language="text")

        # 008 í•„ë“œ ì¶œë ¥ (ë°œí–‰êµ­ ë¶€í˜¸)
        country_code = get_country_code_by_region(location)
        st.code(f"=008  \\$a{country_code}", language="text")
